1.)a.) The network is generally able to adjust correctly and predict the color, but it does depend on where the dot is placed and how many epochs the model is trained for following the new point being added.
   b.) I would say that what is true for adding red dots to a green area, also seems to be true for adding green dots to a red area.
   c.) There is an input layer, two fully connected layers, and a softmax layer.  The input layer reads in the data points.  Each Fully connected layer models activation functions between every node in that layer and every node in the previous layer.  A Softmax layer weights the models outputs based on the probability of each class and assigns a prediction.
   d.) When the number of neurons is increased, the model seems to train faster, but with a less sophisticated decision boundary.  
   e.) I significantly increased the number of layers and the number of neurons in each layer.  Once this is done, no matter where I place a dot, it shortly thereafter is surrounded by a small decision boundary reflecting it's color.  This is likely an overfit as future data is unlikely to exactly repeat a single observation.  Each filter performs 
   f.) RELU converges much faster but with sharper, more jagged decision boundaries.  I can imagine that activation function exacerbating overfitting.  Sigmoid is slowest by a significant amount.
2.)a.) The first layer, it simply brings the data into the network for processing.  The next layer is a convolutional layer.  In this layer, a window slides over the dimensions of the feature space, applying the activation functions in each step, this process is repeated for each filter (8 in this case).  The next layer is a pooling layer; this layer slides a window over the dimensions of the feature space as well, but instead of applying a series of activation functions it typically just finds the maximum value at each step.  The 4th and 5th layers are a repeat of the convolutional and max pooling layers but with slightly different dimensions.  The final layer is a softmax layer that just predicts the likelihood that this observation belongs to each class.  
   b.) It does have an impact on accuracy.  More filters appears to increase the maximum training set accuracy of the model, while fewer filters appears to decrease the time required to train.
   c.) Removing the pooling layers seems to decrease accuracy, but only marginally.
   d.) Yes, adding another convolutional layer seems to have substantially increased accuracy.
   e.) By adding 2 more convolutional layers and 2 more pooling layers and letting the algorithm run a long time, I was able to get an accuracy of more than .99 on the training set.
